{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このドキュメントは？\n",
    "\n",
    "論文「A Contextual-Bandit Approach to　Personalized News Article Recommendation」を読み、その概要のメモするためのもの。\n",
    "\n",
    "この論文は、ニュースサイトの記事レコメンドをBanditアルゴリズムのアプローチで解決し、非常に多くの引用を集めた。\n",
    "\n",
    "\n",
    "### bib\n",
    "\n",
    "``` \n",
    "@inproceedings{li2010contextual,\n",
    "  title={A contextual-bandit approach to personalized news article recommendation},\n",
    "  author={Li, Lihong and Chu, Wei and Langford, John and Schapire, Robert E},\n",
    "  booktitle={Proceedings of the 19th international conference on World wide web},\n",
    "  pages={661--670},\n",
    "  year={2010},\n",
    "  organization={ACM}\n",
    "}\n",
    "```\n",
    "\n",
    "## ABSTRACT\n",
    "\n",
    "パス\n",
    "\n",
    "## 1. INTRODUCTION\n",
    "\n",
    "パス\n",
    "\n",
    "## 2. FORMULATION & RELATED WORK\n",
    "\n",
    "\n",
    "### 2.1 A Multi-armed Bandit Formulation\n",
    "\n",
    "ニュースサイトのようなcontextの情報を扱うバンディットアルゴリズムを、私たちは contextual-bandit algorithm と呼ぶ。\n",
    "\n",
    "例えば、アルゴリズムAでは、試行(tial) t=1,2,3,... と進み、それぞれにtにおいて以下が実行される。\n",
    "\n",
    " 1. ユーザー$u_t$とアーム（表示する記事）セット$A_t$があり、それぞれの持つ特徴ベクトルが$x_{t,a}\\;(a \\in A_t)$とする。これはcontextを表している。\n",
    " \n",
    " 2. 前回の試行($t-1$)の観測に基づき、アーム$a_t \\in A_t$を選択し、報酬$r_{t,a_t}$が得られる。この報酬はユーザーとアームだけに依存すると仮定する。\n",
    " \n",
    " 3. アームを選択する戦術を改善するために、新しい観測値セット($x_{t,a}, a_t, r_{r,a_t}$)を得る。重要なことは、選択されなかったアームの報酬$r_{t,a}$がフィードバックされないことだ。\n",
    " \n",
    " 上記の処理において、試行回数$T$の合計報酬は $\\sum_{t=1}^{T} r_{t,a_t}$ となる。\n",
    " \n",
    " 同様に、最適な合計報酬の予測を $E[\\sum_{t=1}^T r_{t,a_t^*}]$ と定義する。　$a_t^*$ とは報酬が最大と推定されたアームである。\n",
    " \n",
    " この報酬合計を最大化するようにアルゴリズムを設計することが私たちの目標です。\n",
    " つまり、　regretを最小化するようにアーム選択を予測します。\n",
    "ここでは 試行回数Tにおけるregret $R_A(T)$ を以下で公式化します。\n",
    "\n",
    "$$\n",
    "R_A(T) \\overset{def}{=} E[\\sum_{t=1}^{T} r_{t,a_t^*}] - E[\\sum_{t=1}^{T} r_{t,a_t}] \\;\\; ...(1)\n",
    "$$\n",
    "\n",
    "特殊な contextual bandit 問題のケースでは、(i) アームセット $A_t$ は一定でなければなりません、(ii)ユーザー$u_t$ (context($x_{t,1},...,x_{t,K}$)と等価)はあらゆるtで一定です。これはバンディットアルゴリズムと同じであることから、context-free banditと呼びます。\n",
    "\n",
    "一方で、記事レコメンデーションでは、アームとしてプールされた記事を閲覧します。\n",
    "表示した記事がクリックされると、報酬として１が与えられ、そうでなければ０です。\n",
    "この論文では、報酬の予測はクリック率(CTR, Click Throug Rate)となります。\n",
    "そして、このCTRが最大になるように記事を選択することで、合計のクリック数の最大を目指します。\n",
    "\n",
    "幸運なことに、ニュースサイトを運営することで様々な情報を活用できます。\n",
    "例えば、定年後の計画よりもiPod製品に興味があるかなどです。\n",
    "これらの情報をコンパクトにサマライズすることで、バンディットアルゴリズムがCTR予測をできるartcle/user情報を生成しました。\n",
    "これによって、新しい記事やユーザーが現れても、記事をレコメンドできます。\n",
    "\n",
    "\n",
    "### 2.2 Existing Bandit Algorithms\n",
    "\n",
    "UCB(UppeBound) アルゴリズムは、シンプルな$\\epsilon$ -greedyアルゴリズムに対して信頼区間の上側を探索優先度に採用した手法です。\n",
    "ある試行回数時点tで、アクションaの報酬の期待値を$\\hat{\\mu}_{t,a}$と推定し、同時に真の報酬との偏差（信頼区間）を$c_{t,a} > |\\hat{\\mu}_{t,a} - \\mu_{t,a}|$ を算出します。\n",
    "この信頼区間の上側を評価し、$arg \\; max_a (\\hat{\\mu}_{t,a} + c_{t,a})$に基づいてアームを選択します。\n",
    "\n",
    "\n",
    "## 3. ALGORITHM\n",
    "\n",
    "私たちはUCBをベースに、報酬を線形モデルで表現し、信頼区間を閉空間で効果的に計算できるLinUCBと呼ぶアルゴリズムを提案します。\n",
    "はじめにシンプルなモデルを3.1節で紹介し、より一般化した複合モデルを3.2節で紹介します。\n",
    "ただし、LinUCBはニュースのパーソナライズレコメンデーションに適したバンディットアルゴリズムであることに言及しておきます。\n",
    "\n",
    "### 3.1 LinUCB with Disjoint Linear Models\n",
    "\n",
    "d-次元の変数ベクトル$\\mathbf{x}_{t,a}$とそれに対応する係数ベクトル$\\theta_a^*$を使い、その時点tでのアームaの期待報酬を以下のように定式化します。\n",
    "\n",
    "$$\n",
    "E[r_{t,a}|\\mathbf{x}_{t,a}] = \\mathbf{x}_{t,a}^{T} \\theta_a^* \\;\\;...(2)\n",
    "$$\n",
    "\n",
    "このモデルは disjoint であると言えます、なぜならパラメータはアーム（ニュース記事）同士で共有されていないからです。\n",
    "ここで行列$D_a$を定義します。\n",
    "この次元は$(m \\times d)$で、$m$は学習データのインスタンス数であり、そのアームで過去に観測されたコンテキストの数です。\n",
    "そして、その過去のコンテキストに対応する応答ベクトルを$b_a\\in R^m$とします。(つまり、click/no-clickのユーザーフィードバックです)\n",
    "そのアーム（記事）aの過去の表示結果である$(D_a, c_a)$に対して、ridge回帰を適応することで、以下のように係数を推定します。\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_a = (D_a^T D_a + I_d)^{-1} D_t^T c_a \\;\\;...(3)\n",
    "$$\n",
    "\n",
    "$I_d$は$d \\times d$の単位行列です。\n",
    "仮に$c_a$の各要素が互いに独立であるならば、確率$(1- \\delta)$で推定誤差は以下の範囲に収まります。\n",
    "\n",
    "$$\n",
    "|\\mathbf{x}_{t,a}^T \\hat{\\theta}_a - E[r_{t,a}|\\mathbf{x}_{t,a}]| \\leq \\alpha \\sqrt{\\mathbf{x}_{t,a}^T(D_a^T D_a + I_d)^{-1} \\mathbf{x}_{t,a}} \\;\\; ...(4)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\theta > 0 \\;\\; ; \\;\\; \\mathbf{x}_{t,a}^T \\in R^d \\;\\; ; \\;\\; \\alpha = 1+\\sqrt{ln(1/\\delta)/2}\n",
    "$$\n",
    "\n",
    "よって、報酬を線形回帰モデルで予測した場合のUCB型のアーム選択の戦術は、最終的には下記になります。\n",
    "\n",
    "$$\n",
    "a_t \\overset{def}{=} arg \\; \\max_{a \\in A_t} (x_{t,a}^T \\hat{\\theta}_a + \\alpha \\sqrt{x_{t,a}^T A_a^{-1}x_{t,a}}) \\;\\; ...(5)\n",
    "$$\n",
    "\n",
    "$$\n",
    "where \\;\\; A_a \\overset{def}{=} D_a^T D_a + I_d\n",
    "$$\n",
    "\n",
    "信頼区間である(4)は、意図的に(?may be motivated?) 他の原理から派生している。\n",
    "\n",
    "ridge回帰はベイズ推定から考えると、係数の事後分布を$p(\\theta_a)$とし、平均が$\\hat{\\theta}_a$で共分散が$A_a^{-1}$のガウス分布と考えられる。\n",
    "このモデルでは、推定報酬の$x_{t,a}^T \\theta_a^*$の分散は$x_{t,a}^T A_a^{-1} x_{t,a}$であり、その標準偏差は$\\sqrt{x_{t,a}^T A_a^{-1} x_{t,a}}$である。\n",
    "\n",
    "(中略)\n",
    "\n",
    "最後に、私たちは入力される変数ベクトル$x_{t,a}$が、ある正規分布から抽出され、独立であることを仮定した。（式(2)における仮定）\n",
    "他の研究でも似たアプローチをとったものはある。例えばPavlidis et al.などは最小二乗誤差の最適化で係数を求めるアプローチをとっている。\n",
    "\n",
    "次の3.2節では、このアルゴリズムを拡張したもの紹介する。\n",
    "この拡張は、Pavlidis et al.の研究ではカバーされていないものだ。\n",
    "\n",
    "![algorithm1](img/algorithm1.png)\n",
    "\n",
    "\n",
    "### 3.1 LinUCB with Hybrid Linear Models\n",
    "\n",
    "Algorithm 1 では$D_a^T D_a$の逆行列を計算した。\n",
    "$D_a$は行が学習データの変数に対応している。\n",
    "この行列は$d \\times d$の固定的な次元で、増加的に更新されていく。\n",
    "さらにこれらの逆行列は簡単にパラメータとして計算され、式(3)の$\\hat{\\theta}_a$が求められる。\n",
    "これらはdisjintなので、別々に計算される。\n",
    "この章では、hybridモデルを用いたケースを紹介する。\n",
    "\n",
    "多くアプリケーションでは、全てのアームで変数を共通して使うと都合が良い。\n",
    "例えば、新しいニュース記事のレコメンデーションを行うなどだ。\n",
    "この変数の共通化を実現するために、式(2)の右辺を以下のように変更した。\n",
    "\n",
    "$$\n",
    "E[r_{t,a}|x_{t,a}] = z_{t,a}^T \\beta ^ * + x_{t,a}^T \\theta_a^*\n",
    "$$\n",
    "\n",
    "ここで、$z_{t,a} \\in R^k $ は現在のユーザー/記事の変数セットで、$\\beta ^ *$は未知のパラメータ係数で、これは全てのアームが共通して持つ。\n",
    "つまり、アームは共通の$\\beta ^ *$と個別の$\\theta_a^*$の二つの係数を持つことになる。\n",
    "\n",
    "この導入により、Algorithm 1 のような独立したパラメータの計算はできなくなる。\n",
    "幸いにも、UCBを計算する効果的な方法がある。\n",
    "この導出は、block matrix inversion techniques に強く依存する。\n",
    "full-paperではその詳細を記載している。\n",
    "この Algorithm 2　のpseudocodeのみを記載する。\n",
    "5から12行がridge回帰による係数導出で、13行目が係数を再帰的に求めている。\n",
    "\n",
    "![algorithm2](img/algorithm2.png)\n",
    "\n",
    "このアルゴリズムの重要な箇所は、blocksを構築していることで計算を効果的にしている点だ。\n",
    "$(A_0, b_0, A_a, B_a, b_a)$ は全て固定的な次元で、増加的に更新される。\n",
    "さらに、$A_t$にもはや存在しないアーム（記事）については、計算もされない。\n",
    "最後に、逆行列（$A_0^{-1}$, $A_a^{-1}$）を計算しキャッシュ化することで、計算の複雑さを抑制していることだ。\n",
    "\n",
    "\n",
    "## 4. EVALUATION METHODOLOGY\n",
    "\n",
    "Banditアルゴリズムの評価は、本質的にはオンラインで行うべきだが、その実施は難しい。\n",
    "そのため、オフラインdataを使った評価を行うのだが、これはすでに観測済みの範囲でしか評価できないデメリットがある。\n",
    "この問題は強化学習における''off-policy evaluation problem''と呼ばれ、[23]の研究を参照してほしい。\n",
    "\n",
    "一つの解決法として、観測済みのオフラインデータからシミュレーターを構築して、そのシミュレーターが提供する環境下での性能を検証する。\n",
    "しかし、これもシミュレーターの設計に依存する点が問題になる。\n",
    "私たちは他のよりシンプルで過去ログを使った解決法を提案する。\n",
    "\n",
    "(中略)\n",
    "\n",
    "ここに未知の分布 $D$　を過程し、そこから観測データ（tuples)がi.i.d.で取得すると過程します。\n",
    "その観測データは、$(x_1, ..., x_K, r_1, ..., r_K)$ の形式をとり、それぞれは全てのアームの観測された変数と潜在した報酬です。\n",
    "また、大きなサイズのログイベントにアクセスすることを仮定し、それはこの世界でのポリシーの結果であるとします。\n",
    "それぞれのイベントはcontextベクトル$x_1, ..., x_K$で構成され、選択されたアーム$a$によって報酬$r_a$が観測されます。\n",
    "重大なことは、選択されたただ一つのアームに対応する$r_a$は、一様乱数で取得されたアームであることです。\n",
    "単純に説明すれば、私たちは観測済みのログイベントを無限の時間をかけて取得したものと見なし、一方で、私たちは明確なイベントの実際の上限数を設定して、手法を評価します。\n",
    "\n",
    "私たちの目的は、このオフラインデータを使って、バンディットアルゴリズム$\\pi$を評価することです。\n",
    "公式としては、アルゴリズムは時点tに置いてアーム$a_t$を選択するために、（おそらく無作為に）写像されたもので、それまでのイベント履歴$h_{t-1}$と現在のcontextベクトル$x_{t1},...,x_{tK}$に基づいて動作します。\n",
    "\n",
    "私たちの提案する評価方法を Algorithm 3 で示します。\n",
    "\n",
    "![algorithm3](img/algorithm3.png)\n",
    "\n",
    "この手法では、ポリシー$\\pi$(つまりアルゴリズム）と目標の''good''イベントの数$T$を入力します。\n",
    "私たちはログイベントのストリームを一つずつ順番に流していきます。\n",
    "時点tでの履歴$h_{t-1}$が与えられた時、ポリシー$\\pi$が観測したデータと同じアーム$a$を選択した場合は、そのイベントを保存し、履歴に加え、そして合計の報酬$R_t$に加えます。\n",
    "一方で、もしポリシー$pi$が観測されたデータと異なるアームを選択した場合は、このイベント自体を無視して、更新は行わず、次のイベント処理に移ります。\n",
    "\n",
    "注意すべきは、過去に用いたポリシー(logging policy)がアームを一様乱数で選択するものであり、ログイベントはそれによって観測されたものであることです。\n",
    "それによって、それぞれのイベントは全てに対して独立が保証されています。\n",
    "つまり、分布Dから選択されたイベントであるということです。\n",
    "その結果、私たちは二つの処理が等価であることを証明することができます。\n",
    "一つめは分布DからのT回の実測世界(real-world）イベントについての評価。\n",
    "二つめは、それによって観測されたログを提案する手法でのストリーミング評価。\n",
    "\n",
    "これを定理として表現すると、以下のようになります。\n",
    "\n",
    "![theorem](img/theorem.png)\n",
    "\n",
    "\n",
    "(以下は上記の定理の証明。省略。)\n",
    "\n",
    "## 5. EXPERIMENT\n",
    "\n",
    "上記で提案したLinUCBを、Yahoo!のデータに対して、提案したオフライン評価方法で評価しました。\n",
    "\n",
    "### 5.1 Yahoo! Today Module\n",
    "\n",
    "検証に用いるYahoo!　のニュース表示は以下のようなものです。\n",
    "\n",
    "![figure1](img/figure1.png)\n",
    "\n",
    "\n",
    "### 5.2 Experiment Setup\n",
    "\n",
    "このサブセクションでは、検証におけるデータの処理方法などの事前設定について紹介します。\n",
    "\n",
    "#### 5.2.1 Data Collection\n",
    "\n",
    "2009年の5月から、ランダムでデータを収集しました。\n",
    "ある一定の閲覧数のあるユーザーがある確率で選択され、彼女をランダムバケットに入れます。\n",
    "このランダムバケットに入ったユーザーには、ランダムで記事を表示します。\n",
    "バイアスを抑制するために、評価ではF1に表示された記事しか用いません。\n",
    "\n",
    "5月1日にニュースを見た470万人のユーザーをランダムバケットに入れ、この1日のデータ(これをtunning dataと呼びます)を学習データとして、バンディットアルゴリズムのパラメータとします。\n",
    "そして、その学習結果に対して、次の一週間のデータ（これをevaluation dataと呼びます）をテストにして、評価を行います。\n",
    "\n",
    "#### 5.2.2  Feature Construction\n",
    "\n",
    "ここでは、ユーザーと記事の変数の生成方法について説明します。\n",
    "disjointモデル(algorithm1)とhybridモデル(algorithm2)の二種類の変数セットがあります。\n",
    "\n",
    "ユーザーの変数をそのまま利用することからはじめていきましょう。\n",
    "変数はsupport(その変数を持っているユーザーの割合)が0.1よりも高いものだけを使います。\n",
    "これにより、ユーザーは約1000個のカテゴリカル変数で表現可能になります。\n",
    "具体的な変数として: (i)デモグラフィック; (ii) 住所情報; (iii) 行動情報;　があります。\n",
    "(iii)はYahoo! の各プロパティの閲覧行動を変数化したものです。\n",
    "\n",
    "同様に、記事の変数についても説明します。\n",
    "(i)URLカテゴリ; (ii)編集カテゴリ; で、約100個のカテゴリカル変数です。\n",
    "(ii)は記事の編集者が設定するサマライズタグです。\n",
    "\n",
    "これらの変数をバイナリ化して、変数を標準化します。\n",
    "この処理については、[12]の研究を参考にしました。\n",
    "これらによって、記事は83、ユーザーは1193のエンティティで表現されました。\n",
    "\n",
    "また、変数の次元圧縮のために、[13]の研究を参考にして、2008年9月のユーザーのニュースカテゴリの閲覧行動からユーザー同士の共通行動を解析してクラスタ化しました。\n",
    "より詳細については下記になります。\n",
    "\n",
    "![detail1](img/detail1.png)\n",
    "\n",
    "上記の処理により、施行t時点で選択された記事aは6次元の変数ベクトル$x_{t,a}$で表現され、ユーザーも同様の6次元の変数ベクトル$u_t$で表現されます。\n",
    "この場合、記事の変数は共有化されていないので、これは3章で定義したdisjointの線形回帰モデルだと言えます。\n",
    "\n",
    "この$x_{t,a}$と$u_t$の外積によって$6 \\times 6 =36$の値を取得できます。これをhybridモデル(式(6))で利用する記事で共有される変数ベクトル$z_{t,a}$とします。\n",
    "よって$(z_{t,a}, x_{t,a})$はhybridの線形モデルで利用されます。\n",
    "$z_{t,a}$はユーザーと記事の相互作用変数で, $x_{t,a}$はユーザーのみの変数でもあります。\n",
    "\n",
    "私たちはユーザーと記事を5つの共通クラスタ変数と1つの切片の6次元に変換しました。\n",
    "この数については先行研究[13]を参考にしたものですが、同時にwebの掲載を行う上で現実的な処理速度を担保するためでもあります。\n",
    "\n",
    "\n",
    "### 5.3 Compaired Algorithms\n",
    "\n",
    "対象実験に用いるアルゴリズムを紹介します。\n",
    "\n",
    "##### Ⅰ. 変数を取らないアルゴリズム\n",
    "\n",
    "context free 型のアルゴリズムです。\n",
    "\n",
    "1. random\n",
    "2. $\\epsilon - greedy$\n",
    "3. ucb\n",
    "4. omniscient\n",
    "\n",
    "##### Ⅱ. 'warm start' のあるアルゴリズム\n",
    "\n",
    "パーソナライゼーションへの前提段階のアルゴリズムで、ユーザーと個別記事のCTRを事前に学習してからcontext free型のアルゴリズムを実行する、というアイデアです。\n",
    "先行研究[12]に基づいたロジスティック回帰モデルを用いました。\n",
    "\n",
    "1. $\\epsilon - greedy(warm)$\n",
    "2. $ucb(warm)$\n",
    "\n",
    "##### Ⅲ. Algorithms that learn user-specific CTRs online\n",
    "\n",
    "1. $\\epsilon - greedy(seg)$ : ユーザーを単純に5つのsegmentに分けて処理します(seg)。\n",
    "2. $ucb(seg)$\n",
    "3. $\\epsilon - greedy(disjoint)$\n",
    "4. $\\epsilon - greedy(hybrid)$\n",
    "5. $linucb(hybrid)$\n",
    "\n",
    "\n",
    "### 5.4 Performance Metric\n",
    "\n",
    "これから、各手法のCTRを比較するが、ランダム選出手法のCTRの何倍のCTRになったかを報告する。\n",
    "以降でCTRと記載があれば、ランダムとの相対的なCTR比率であることに注意。\n",
    "\n",
    "Yahoo! へのトラフィックを、[3]の研究を参考にして、２つのバケットに分ける。\n",
    "一つは''learning bucket''である。一般的にはトラフィックのうち、ごく少数が割り振られ、各種のバンディットアルゴリムが学習するためのデータとして使われる。\n",
    "もう一つは''deployment bucket''で、これは学習されたCTRを用いてYahoo!がユーザーにコンテンtぬを表示する通常のFrontPageである。\n",
    "learning bucketでのデータが更新されれば、各手法は学習され記事の予測CTRが更新される。\n",
    "いずれのbucketでの性能も、algorithm3の手法で評価した。\n",
    "\n",
    "### 5.5 Experimental Results\n",
    "\n",
    "#### 5.5.1 Results for Tuning Data\n",
    "\n",
    "ucbの場合は$\\alpha$, $\\epsilon$-greedyの場合は$\\epsilon$のチューニングが必要になる。\n",
    "tunning data　を用いて決定した結果は以下の通りです。\n",
    "\n",
    "![figure2](img/figure2.png)\n",
    "\n",
    "以下が、各手法のCTRを比較した表である。\n",
    "\n",
    "![table1](img/table1.png)\n",
    "\n",
    "deployment bucket のデータ量は大きいため、そこで高いCTRを出すことが重要だが、learning bucketでの高いCTRは学習効率が良いことを示している。両方とも重要な指標だ。\n",
    "\n",
    "上の図の％は、$\\epsilon$-greedyをベースラインにしたCTRの改善度(lift)である。\n",
    "size=は、パラメータチューニングに用いたデータ量を表している。\n",
    "sizeが少ないほどスパースな状況での性能を表している。\n",
    "\n",
    "\n",
    "#### 5.5.2 Results for Evaluation Data\n",
    "\n",
    "##### On the Use of Features\n",
    "\n",
    "ベースラインに比べて10%近いCTR改善が見られるため、提案手法は有効なニュース記事のパーソナルレコメンデーションである。\n",
    "\n",
    "より詳細なCTR改善を可視化したものが、下記のFigure3である。\n",
    "このグラフはベースモデルのCTR(base ctr)からの改善CTR(lifted ctr)を比較したもので、赤い十字記号がベースである$\\epsilon$-greedyであり、青い丸点が比較する手法である。\n",
    "それぞれ、最も多く閲覧された50個の記事ごとに評価CTRをプロットしている。\n",
    "\n",
    "提案手法であるlinucbは、ユーザーの情報を学習しない手法に比べて、記事ごとに改善が確認できる。\n",
    "\n",
    "![figure3](img/figure3.png)\n",
    "\n",
    "\n",
    "##### On the Size of Data\n",
    "\n",
    "記事の数は非常に多いため、CTR評価を行う deployment backet における表示回数は少ない方が良い。\n",
    "\n",
    "Table1　でも、size=[30,20,10,5,1%]ごとにデータサイズでの性能を比較し、deployment backetでのCTRの高さを評価している。\n",
    "\n",
    "より良い可視化として、Figure4を作成した。\n",
    "データのスパースレベルごとの、各アルゴリズムの性能を比較したものだ。\n",
    "どのレベルでも各手法は機能している。\n",
    "スパースレベルが1%出会っても、linucbがucbを上回っている。\n",
    "\n",
    "また、ucbが$\\epsilon$-greedyよりも、スパース状況におけるdeployment backetでのCTR(=学習速度)に優れていた。この傾向はよりスパースな状況で、さらに顕著になる。\n",
    "\n",
    "さらに、ucb(seg), linucb(disjoint), linucb(hybrid)の性能の違いがスパースになるにつれ顕著になっている。\n",
    "linucb(hybrid)がデータの少ない状態で高い性能を発揮できているのは、記事の共通の変数をもち、学習結果を転用しているためだと考えられる。\n",
    "\n",
    "\n",
    "![figure4](img/figure4.png)\n",
    "\n",
    "\n",
    "![figure5](img/figure5.png)\n",
    "\n",
    "\n",
    "\n",
    "## 6. CONCLUSIONS\n",
    "\n",
    "pass\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
