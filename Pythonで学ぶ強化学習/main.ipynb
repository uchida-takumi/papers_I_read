{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# このドキュメントの目的\n",
    "\n",
    "強化学習（Reinforce Learning)について、基礎的な知識を確認します。\n",
    "\n",
    "\n",
    "## 参考：URL\n",
    "\n",
    "http://blog.brainpad.co.jp/entry/2017/02/24/121500\n",
    "\n",
    "https://qiita.com/icoxfog417/items/242439ecd1a477ece312\n",
    "\n",
    "## 読んだ本\n",
    "\n",
    "久保隆宏. 機械学習スタートアップシリーズ　Ｐｙｔｈｏｎで学ぶ強化学習　入門から実践まで (Japanese Edition)\n",
    "\n",
    "https://www.amazon.co.jp/%E6%A9%9F%E6%A2%B0%E5%AD%A6%E7%BF%92%E3%82%B9%E3%82%BF%E3%83%BC%E3%83%88%E3%82%A2%E3%83%83%E3%83%97%E3%82%B7%E3%83%AA%E3%83%BC%E3%82%BA-%EF%BC%B0%EF%BD%99%EF%BD%94%EF%BD%88%EF%BD%8F%EF%BD%8E%E3%81%A7%E5%AD%A6%E3%81%B6%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92-%E5%85%A5%E9%96%80%E3%81%8B%E3%82%89%E5%AE%9F%E8%B7%B5%E3%81%BE%E3%81%A7-%EF%BC%AB%EF%BC%B3%E6%83%85%E5%A0%B1%E7%A7%91%E5%AD%A6%E5%B0%82%E9%96%80%E6%9B%B8-%E4%B9%85%E4%BF%9D%E9%9A%86%E5%AE%8F-ebook/dp/B07NVKFZP5/ref=sr_1_1?__mk_ja_JP=%E3%82%AB%E3%82%BF%E3%82%AB%E3%83%8A&keywords=%E5%BC%B7%E5%8C%96%E5%AD%A6%E7%BF%92&qid=1556956448&s=digital-text&sr=1-1\n",
    "\n",
    "\n",
    "読んだ本の簡単な解説つきGithub（以下を読んだ方がまとまっている）\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 強化学習の位置づけを知る\n",
    "\n",
    "## 強化学習の立ち位置\n",
    "\n",
    "機械学習を学習に用いるデータへの態度から分類すると、以下の３つが存在する。\n",
    "\n",
    "- 教師あり学習：　　すでに観測された正解つきデータを扱う\n",
    "- 教師なし学習：　観測されているが正解のないデータを扱う\n",
    "- 強化学習：　データを観測し正解を得ながら学習する\n",
    "\n",
    "強化学習が活躍する状況は、予測を行う上で観測しなければならないデータが多いが全てを観測するのが困難な状況であると言える。\n",
    "また、次の観測すべきデータを随時選びながら学習を行う挙動をとることから、囲碁や将棋のAIや広告IDの配信最適化などに広く応用されている。\n",
    "\n",
    "## 強化学習の特徴\n",
    "\n",
    "強化学習は、以下の２つのモデルが必要になります。\n",
    "\n",
    "1. 行動（アクション）の評価方法\n",
    "2. 評価方法に基づいた、行動の選び方（＝戦略、ポリシー）\n",
    "\n",
    "1は多くの場合、獲得する累積報酬が最大になるような評価方法を設計します。\n",
    "2は多くの場合、1で定義された評価方法に基づいて、累積報酬が最大になるように設計します。\n",
    "\n",
    "よって、強化学習とはモデリングで設計された以上の性能は発揮できません。これは正解レベルのない「教師なし学習」が抱える問題ーーその推定結果はどのくらい正しいの？　と同様の問題を持っています。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習の基礎用語\n",
    "\n",
    "代表的な強化学習の一つであるBanditアルゴリズムでは、例えば、カジノにある複数のスロットからどのアームを引けば最も高い報酬が得られるかを探索する。\n",
    "\n",
    "また、広告IDの配信最適化では、広告枠をブラウジングしているユーザーIDに応じて、どの広告IDを表示すればクリックを最大化できるかを探索する。\n",
    "\n",
    "いずれも、報酬率の高いアクションの選択問題である。\n",
    "\n",
    "このような共通する問題構造に対して、強化学習ではいくつかの専門用語を定義している。それを以下で解説する。\n",
    "\n",
    "#### 環境(environment)\n",
    "カジノ施設、広告を表示するブラウザ（＝ユーザーID）などに相当する。\n",
    "\n",
    "#### エージェント(agent）\n",
    "その環境で報酬を得るためにアクションを実行する主体。\n",
    "カジノならプレイヤー、広告配信を行う広告主が相当する。\n",
    "\n",
    "#### アクション(action): a\n",
    "Banditアルゴリズムではスロットを引く「アーム」とも呼ばれる。\n",
    "広告配信ならば配信する広告ID。\n",
    "\n",
    "#### 報酬(reward): r\n",
    "スロットならば「当たり」、あるいは「獲得金額」。\n",
    "広告配信ならば配信結果でクリックされたかどうかの{0,1}。\n",
    "その時点の行動で直接得られる報酬を「即時報酬」という。\n",
    "\n",
    "#### 状態(state): s\n",
    "その環境の状態、あるいはエージェントの状態。\n",
    "\n",
    "#### 時間(time あるいは tick): t\n",
    "アクションの実行を判断するタイミング。その時の状況を $s_t$ , 選択したアクションを $a_t$ と表現します。\n",
    "\n",
    "#### 即時報酬（報酬関数、Reward function）:R\n",
    "状態と遷移先を引数に、報酬を出力する関数。（行動を引数に取ることもある）\n",
    "$$\n",
    "R(s_{t}, s_{t+1})=r_t\n",
    "$$\n",
    "$$\n",
    "R(s_{t}, a_{t})=r_t\n",
    "$$\n",
    "\n",
    "![img1-9](img/img1-9.png)\n",
    "\n",
    "#### 状態遷移の確率（遷移関数、Transition function）: T\n",
    "状態と行動を引数に、遷移先（次の状態$s_{t+1}$）と遷移確率を出力する関数。\n",
    "\n",
    "#### エピソード(Episode)\n",
    "強化学習に与えられた環境の開始から終了までの期間。永遠に続く（終了が無限）の場合もある。\n",
    "\n",
    "#### 価値: Q(s, a)\n",
    "その状況sで、そのアクションaを選択した場合に得られる長期的な価値（報酬ではない事に注意）。\n",
    "例えば、環境としてオセロを想定した場合、即時報酬はその時点で置いた石がひっくり返した相手の石の数となる。\n",
    "しかし、次の相手の手によっては、長期的に不利な状況に陥ってしまう可能性がある。\n",
    "それを考慮するために価値と呼び、報酬とは定義を分けている。\n",
    "\n",
    "これをQ値と呼ぶこともある。\n",
    "素朴なポリシーでは、エージェントはQ(s,a)が最大になるaを選択して実行します。\n",
    "\n",
    "#### ポリシー(policy): $\\pi(s) = a$\n",
    "戦略。その状況sにおいてどのような行動aを取るべきかの定義。プログラミングでは関数として定義される。\n",
    "\n",
    "### (補足)微妙に分かりやすい用語説明\n",
    "\n",
    "![good_explain](img/good_explain.png)\n",
    "\n",
    "### (補足)もっと分かりやすいゲームの事例\n",
    "\n",
    "Metacar（https://www.metacarproject.com/qtable.html）\n",
    "\n",
    "久保隆宏. 機械学習スタートアップシリーズ　Ｐｙｔｈｏｎで学ぶ強化学習　入門から実践まで (Japanese Edition) (Kindle の位置No.233-234). Kindle 版. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# シンプルな強化学習：Markov Decision Process\n",
    "\n",
    "シンプルな強化学習の例として、マルコフ性（直前の状態だけに遷移先が依存する）を仮定した環境、Markov Decision Process(MDP)での強化学習を考えてみます。\n",
    "\n",
    "これは、つまり、上述でも表示した以下のような環境です。\n",
    "\n",
    "![img1-9](img/img1-9.png)\n",
    "\n",
    "上図のロボットは、エージェントであり戦略（ポリシー）$\\pi$という関数でもあります。これはプログラミングでも関数として表現されます。（エージェントが複数のポリシーを状態によって使い分ける設計もあり得ますが、ここではシンプルに一つのポリシーのみを考慮します）\n",
    "\n",
    "時点tで得られているデータから推定する報酬rの総和$G_t$の最大化を考えます。しかし、時点tから離れた時点（例えば、t+10）ほど推定精度が落ちると考えます。この未来の推定精度悪化を割引率$\\gamma$を導入して、以下のようにモデルを設計します。\n",
    "\n",
    "![G_t](img/G_t.png)\n",
    "\n",
    "この報酬rの推定総和$G_t$を「期待報酬（Expected reward）」と言い、ここではこれを価値(Value)として扱います。以降では価値と呼びます。\n",
    "\n",
    "この価値を評価することを「状態評価（Value approximation）」と呼び、これがMDP環境における強化学習の特徴的な一つ目のモデル「行動の評価方法」に相当します。\n",
    "\n",
    "ここからは、MDPの実装例をみていきます。\n",
    "\n",
    "MDPの環境として、下図のような迷路を想定しました。エージェントが上下左右に行動し、緑に到達すればプラスの報酬、赤に到達したらマイナスの報酬を得ます。黒いセルには移動できません。\n",
    "\n",
    "![MDP_environment](img/MDP_environment.png)\n",
    "![MDP_environment2](img/MDP_environment2.png)\n",
    "\n",
    "この環境の実装は、以下のgithubで掲載されています。\n",
    "\n",
    "環境、アクション、報酬などの全体的な定義プログラム：\n",
    "\n",
    "　　　　https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/DP/environment.py\n",
    "\n",
    "上記の定義プログラムの実行デモのプログラム：\n",
    "\n",
    "　　　　https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/DP/environment_demo.py\n",
    "\n",
    "また上記のコードが実現する処理フローの図が下記になります。\n",
    "\n",
    "![MDP_environment3](img/MDP_environment3.png)\n",
    "\n",
    "\n",
    "コードの詳細については、直接読むのが一番良いと思います。\n",
    "\n",
    "\n",
    "# ここまでのまとめ\n",
    "\n",
    "上述のコードを読むと納得すると思いますが、強化学習とは報酬評価モデルと行動選択モデルの組み合わせで成立しています。これらのモデルはプログラミングで実装することが多いです。\n",
    "\n",
    "この他にも、環境や報酬の定義もプログラミングする必要があります。\n",
    "\n",
    "ゲーム的な環境は実装例が多いですが、例えばWebマーケティングの実データを読み込んだ環境を定義し、実際に広告配信を最適化している事例もあります。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習のより複雑なモデリング\n",
    "\n",
    "$Q(s,a)$ が分かっているならば、それが最大になるaを選択するだけで良い。\n",
    "\n",
    "しかし、実際はどのアクションaが、どの状態sの時に、どの程度の報酬rが期待できるかは分からない。エージェントは、その状況$s_t$で、実際にアクション$a_t$を選択し、結果である報酬$r_t$を観測することで、$Q(s,a)$を学習する必要がある。\n",
    "\n",
    "さらに「前回の行動が、今回の行動の報酬に影響を与えている」ことが想定される場合は、より複雑なモデルで$Q(s,a)$を定義する必要がある。\n",
    "\n",
    "このような複雑な仮定をおくのは、強化学習が元々はチェスや将棋などの状況を想定しているのが理由かもしれない。\n",
    "\n",
    "この考え方を、数式で表現すると以下のようになる。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) = E_{s_{t+1}}(r_{t+1} + \\gamma E_{a_{t+1}}(Q(s_{t+1},a_{t+1})))  \\:\\:  ...(1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "E_{s_{t+1}}(...) : (...)をs_{t+1}で周辺化した期待値。\n",
    "$$\n",
    "$$\n",
    "r_{t+1} : 即時報酬。\n",
    "$$\n",
    "$$\n",
    "\\gamma : 割引率。将来の価値の重みを設定するパラメータ。\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 強化学習のアルゴリズム\n",
    "\n",
    "学習の目的は、式(1)のt+1時点の期待値を推定することにある。\n",
    "\n",
    "この時、実際に行動を実施しながらQ値を更新していく。\n",
    "つまり、実際に構想した結果をサンプルとして期待値を推定する。\n",
    "\n",
    "このようなシンプルなアルゴリズムとして、「Q学習」「Sarsa」「モンテカルロ法」などがあります。\n",
    "\n",
    "### Q学習\n",
    "\n",
    "式(1)の　$r_{t+1} + \\gamma E_{a_{t+1}}(Q(s_{t+1},a_{t+1}))$ を、$r_{t+1} + \\gamma \\max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))$ に置き換えることで、Q値を以下のように更新します。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) ←  (1-\\alpha)Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))) \\:\\: ...(2)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\alpha : 学習率(0.0〜1.0)。調整パラメータ。\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Day2: 強化学習の解法(1) ：環境から計画を立てる\n",
    "\n",
    "\n",
    "ここでは動的計画法（Dynamic Programming:DP)を用いた学習方法を解説します。\n",
    "\n",
    "Day1で実装した迷路の環境は、遷移関数と報酬関数が明らかであり、これを「モデルベース」の学習方法と呼びます。モデルとは遷移関数と報酬関数の２つのことです。\n",
    "\n",
    "Day2の論点は以下になります。\n",
    "\n",
    "- 行動評価の指標となる「価値」の定義\n",
    "- 「状態評価」を動的計画法で学習する手法と実装方法\n",
    "- 「戦略」を動的計画法で学習する手法と実装方法\n",
    "- モデルベースの手法（Day1）とDay3の手法であるモデルフリーの手法の違い\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 価値の定義と算出：Bellman Equation\n",
    "\n",
    "Day1で定義した「価値」は、未来の報酬$r_{t+k}$が明らかである必要があります。しかし、現実としてそれは不明です。具体的に言うと、コイントスゲームで3回後のコインは表が出ることがすでに分かっている必要があります。\n",
    "\n",
    "しかし、実際には未来のコイントスの裏/表は確率的です。\n",
    "\n",
    "「価値$G_t$」のモデル式で表すと下記になりました。\n",
    "\n",
    "$$\n",
    "G_t = r_{t+1} + \\gamma G_{t+1} \n",
    "$$\n",
    "\n",
    "観測可能である即時報酬$r_{t+1}$と、再帰的に表現される将来の即時報酬$G_{t+1}$と割引率$\\gamma$で構成されます。\n",
    "\n",
    "現実的に将来の報酬が確率的であることを考慮するためには$G_{t+1}$を工夫する必要があります。\n",
    "\n",
    "動的計画法では、$G_{t+1}$を過去の計算値（キャッシュ）を使い、「メモ化」と呼ばれる処理を施すことで、これに対応します。\n",
    "\n",
    "この際、即時報酬を確率的に取り扱うために期待値で考えます。つまり、裏/表の確率が0.5/0.5で、その即時報酬が10円/１００円だった場合、その期待値は$10*0.5+100*0.5=45円$です。\n",
    "\n",
    "また、エージェントが選択する行動について考えましょう。以下の２種類があります。\n",
    "\n",
    "- （Policyベース）エージェントは保持している戦略に基づき行動する。\n",
    "- （Valueベース）エージェントは常に「価値」が最大になる行動を選択する。\n",
    "\n",
    "この分類の意味は、「行動の評価方法」のみを学習するか（Valueベース）、それだけでなく「行動の選び方（戦略）」も学習するか（Policyベース）の違いです。\n",
    "\n",
    "Policyベースの場合、戦略$\\pi$に基づいて、状況$s$で行動$a$をとる確率は$\\pi(a|s)$です。遷移先の状態$s'$へ移る確率は遷移関数$T(s'|s,a)$で定義されます。\n",
    "\n",
    "![fig2.1.png](img/fig2.1.png)\n",
    "\n",
    "戦略$\\pi$に基づいた結果得られる価値を$V_{\\pi}(s)$とすると、これも再帰的に定義できます。\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s_t) = E_{\\pi}[r_{t+1} + \\gamma V_{\\pi}(s_{t+1})] \\;\\;\\; Eは期待値\n",
    "$$\n",
    "\n",
    "これまで定義してきた要素を代入し、報酬を報酬関数$R{s,s'}$で書き直すと、以下のようにモデルが詳細になります。\n",
    "\n",
    "$$\n",
    "V_{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'}T(s'|s,a)(R(s,s') + \\gamma V_{\\pi}(s')) \n",
    "$$\n",
    "\n",
    "上記の式を、Bellman Equationと呼び、これによって以下を（関数としてプログラミングで）定義できれば、その状態におけるその戦略$\\pi$の将来価値$V_{\\pi}(s)$を評価できます。\n",
    "\n",
    "つまり、その状況$s$でどの戦略$\\pi$を選択すべきかを、$V_{\\pi}(s)$を比較することで決定することができます。\n",
    "\n",
    "* $V_{\\pi}(s)$ の計算に必要な定義（関数として任意にプログラミング）\n",
    "\n",
    "- $\\pi(a|s)$  ポリシーの定義\n",
    "- $T(s'|s,a)$  状態遷移の定義（$s'$は次の状態）\n",
    "- $R(s,s')$  報酬の定義\n",
    "\n",
    "ここで、$V_{\\pi}(s)$のプログラミングとしての実装で、再帰的な処理を実現する必要があります。\n",
    "それについては、この書籍で挙げられている実装やgithubで公開されているコードを確認してください。\n",
    "また、後半に出てくる動的計画法を用いて近似値で対応することもできます。\n",
    "\n",
    "「価値」が最大になる行動を常に選択するケースでは、戦略$\\pi$を考慮しませんが、Bellman Equationの変換で対応できます。(＊書籍の数式に間違いがあると思われたので、勝手に修正)\n",
    "\n",
    "$$\n",
    "V(s) = \\max_{a} \\sum_{s'} T(s'|s,a) (R(s, s') + \\gamma V(s'))\n",
    "$$\n",
    "\n",
    "さらに、単純に報酬が現在の状態のみで決まる場合（つまり、$R(s)$の場合）は、以下のように単純化されます。\n",
    "\n",
    "$$\n",
    "V(s) = R(s)　+ \\gamma \\max_{a} \\sum_{s'} T(s'|s,a)  V(s')\n",
    "$$\n",
    "\n",
    "例えば、Day1の迷路のケースでは、現在地の座標(状況$s$)によってのみ即時報酬が決定されるので、$R(s)$で計算することが可能です。\n",
    "\n",
    "以降では、このケースを想定して実装を確認します。\n",
    "\n",
    "### プログラムでの実装例\n",
    "\n",
    "下記のプログラムの関数を参照。\n",
    "\n",
    "アクションが{'up', 'down'}の二種類しかなく、５回までのエピソードで、4回以上'up'すると報酬を獲得できる環境を想定している。\n",
    "\n",
    "ここで、$V(s)$を計算する際の再帰的な処理例も紹介されている。\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/DP/bellman_equation.py\n",
    "\n",
    "\n",
    "### Bellman Equation の欠点\n",
    "\n",
    "それは、$V(s')$が計算済みである必要があることです。\n",
    "\n",
    "しかし、多くの環境では状態数は非常に多く、しらみつぶしの計算は困難です。\n",
    "\n",
    "そこで、動的計画法（Dynamic Programming)を用います。\n",
    "$V(s')$に適当な値を設定し、複数回計算を繰り返すことで精度を上げます。\n",
    "モデルに再帰的な定義をしているため、この動的計画法が応用可能なのです。\n",
    "\n",
    "動的計画法では、複数回の計算で最適解への近似が可能であることが証明されています。\n",
    "それが気になる人は、\"UCL Course on RL\" の Lecture 3 を確認してください。\n",
    "\n",
    "UCL Course on RL：Lecture 3）https://www.youtube.com/watch?v=Nd1-UUMVfz4\n",
    "\n",
    "↓↓↓\n",
    "\n",
    "![RL01.png](img/RL01.png)\n",
    "![RL02.png](img/RL02.png)\n",
    "![RL03.png](img/RL03.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 動的計画法による状態評価の学習: Value Iteration\n",
    "\n",
    "動的計画法により価値を再帰的な計算によって算出する方法を、価値反復法（Value Iteration）と呼びます。価値Vを、$V_1 →　V_2  →　 ...$ と繰り返していくことでその精度を上げていきます。\n",
    "\n",
    "$$\n",
    "V_{i+1}(s) = max_{a} {\\sum_{s'} T(s'|s,a)(R(s)+\\gamma V_i(s'))}\n",
    "$$\n",
    "\n",
    "i+1回目の計算結果$V_{i+1}$は、前回の計算結果$V_i$を利用して計算されます。$s'$は次の状態を表しています。\n",
    "\n",
    "これは繰り返し計算ですから、収束を見極めて中断判断が必要になります。一般的には前後の差異を比較して、変化量が一定値以下であれば打ち切ります。\n",
    "\n",
    "### 実装例\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/DP/planner.py\n",
    "\n",
    "クラス Planner がその実装となります。\n",
    "\n",
    "Planner.transitions_atは遷移関数$T(s'|s,a)$の実装です。\n",
    "同時に遷移先で得られる報酬も返却します。\n",
    "返却はreturn ではなく yieldで定義されているため、Planner.transitions_atはジェネレーターとして使用されます。\n",
    "つまり、for p,n,r in Planner.transitions_at(state, action): として一つずつ読み出すことが可能です。\n",
    "\n",
    "クラス　ValueIterationPlanner は Planner　を継承して定義されたクラスです。\n",
    "ValueIterationPlanner.planが処理の中心であり、変数Vは描く状態の期待報酬で、Iterationごとに更新されていきます。\n",
    "Vの初期値はすべての状態を0とします。\n",
    "この時、遷移先の価値Vは前回計算したVから引用しています。（V[next_stage]）\n",
    "\n",
    "詳細まで理解するためには、上述で紹介したUCL Course on RL：Lecture 3の講座動画を見た上で、実際の実装コードをよく見ると効率的です。\n",
    "\n",
    "また、以下のプログラムを実行することで、アルゴリズムの動きが体感できるデモンストレーションアプリをブラウザ上で表示することができます。\n",
    "\n",
    "```\n",
    "git clone https://github.com/icoxfog417/baby-steps-of-rl-ja.git\n",
    "cd baby-steps-of-rl-ja\n",
    "python DP/run_server.py\n",
    "\n",
    "> ブラウザで http://localhost:8888/ にアクセスする。\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 動的計画法による戦略の学習：Policy Iteration\n",
    "\n",
    "エージェントが戦略（Policy)に基づいて行動することをPolicyベースと言います。\n",
    "\n",
    "戦略はその状態における行動確率を出力します。(　$\\pi(a|s)$　)　この行動確率から価値（期待値）を計算します。\n",
    "\n",
    "このように、戦略により価値を計算し、また価値を最大化するように戦略を更新する、と言うプロセスを繰り返すことで価値（状態の評価 $V(s)$ ）と戦略の精度を高めていきます。\n",
    "\n",
    "この繰り返しのプロセスを Policy Iteration と呼びます。\n",
    "\n",
    "### 実装例\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/DP/planner.py\n",
    "\n",
    "上記のクラス　PolicyIterationPlanner　がその実装になります。\n",
    "\n",
    "PolicyIterationPlanner.initialize ではpolcyの初期化が定義されており、一様分布を割り当てています。\n",
    "\n",
    "PolicyIterationPlanner.estimate_by_policy ではPolicy Iterationでの価値の計算を行います。ValueIterationPlanner.estimate_by_policyと違い、\"r += ...\" の更新部分にaction_probがあります。これは、Value Iterationが価値が最大の行動を必ず選ぶのに対して、Policy Iterationでは戦略に基づいて（＝確率的に）行動を選択するからです。\n",
    "\n",
    "estimate_by_policy は動的計画法に基づいて価値$V(s)$を更新していき、更新幅がthreshold以下の場合に繰り返し処理を中断し、Vを返却します。\n",
    "\n",
    "計算されたVは、戦略の評価に使用されます。この点をもって、戦略による価値の計算（estimate_by_policy）を「戦略評価（Policy Evaluation）」とも呼びます。\n",
    "\n",
    "その評価を行なっているのが、　PolicyIterationPlanner.plan です。\n",
    "\n",
    "PolicyIterationPlanner.plan では、estimate_by_policyで計算されたVを用いて、各行動の価値を計算します。（r += prod * (reward + gamma * V[next_state])）。そこから、最も価値の高い行動を best_action　が常に選択されるように self.policy（$\\pi(a|s) → prob$） が管理している確率を更新(self.policy[s][a] = prob)します。\n",
    "\n",
    "このように、Policy Iteration では、価値の計算（estimate_by_policy)と戦略の更新(self.policy[s][a]=prob)を繰り返します。この相互更新こそがPolicy Iterationの中核です。つまり「状態評価（V）」と「戦略（self.policy）」双方が学習されます。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Day2 の復習\n",
    "\n",
    "Day2では、「価値の定義」「状態評価の学習」「戦略の学習」の３つを学習しました。\n",
    "\n",
    "#### 「価値の定義」\n",
    "\n",
    "Day1で解説した環境では２つの問題がありました。すなわち、１）将来の即時報酬の値が明らかである必要があり、２）それが必ず得られる必要もある、です。\n",
    "１）は式の再帰的な定義で解決し、２）は確率を導入し期待値として表現することで解決しました。\n",
    "この２つの解決アプローチを表現したモデル式を「Bellman Equation」と呼びました。\n",
    "\n",
    "#### 「状態評価の学習」のみを行う事を、Valueベースという。\n",
    "\n",
    "#### 「状態評価の学習」と「戦略の学習」を行う事を、Policyベースと言う。\n",
    "\n",
    "実際にその学習を行う方法として、動的計画法を学びました。\n",
    "動的計画法のポイントはメモ化であり、再帰的な計算箇所に過去の計算結果（メモ）を利用します。\n",
    "そのため、Bellman Equationを計算することができます。\n",
    "\n",
    "Day2 の最後として、モデルベースとモデルフリーの違いについて解説します。\n",
    "\n",
    "## 2.4 モデルベースとモデルフリーの違い\n",
    "\n",
    "Day2 で紹介してきた手法はモデルベースです。\n",
    "\n",
    "これは遷移関数と報酬関数が明らかである必要があります。\n",
    "\n",
    "しかし、現実的には遷移関数と報酬関数が不明で、実際にエージェントを動かしてデータをアップデートしながら、報酬関数や遷移関数を調整する必要があります。\n",
    "\n",
    "これをモデルフリーの手法と言います。\n",
    "\n",
    "モデルフリーの方が現実的だと思われるかもしれませんが、昨今ではあらかじめ定義したモデルを設定できるモデルベースが見直されている実感があります。\n",
    "\n",
    "なんにせよ、Day3ではモデルフリーの手法について紹介していきます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Day3\n",
    "\n",
    "\n",
    "## 強化学習の解放(2) :経験から計画を立てる\n",
    "\n",
    "Day3ではモデルフリーの手法について解説します。\n",
    "モデルフリーでは、遷移関数と報酬関数は「わかっていない」ことが前提となります。\n",
    "\n",
    "ここでは検討すべきポイントが３つあります。\n",
    "\n",
    "1. 経験の蓄積と活用のバランス\n",
    "2. 計画の修正を実績から行うか、予測で行うか\n",
    "3. 経験を状態評価、戦略どちらの更新を利用するか\n",
    "\n",
    "![img/fig3.1.png](img/fig3.1.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 経験の蓄積と活用のバランス：Epsilon-Greedy法\n",
    "\n",
    "環境の情報（つまり、遷移関数と報酬関数）が未知の場合、エージェントが行動することで関数を明らかにしていく工程（探索）と推定した関数に基づいて報酬を最大化する工程（活用）に分けられます。\n",
    "\n",
    "どちらを重視するかの「探索と活用のトレードオフ」（Exploration-exploitation trade-off）があります。\n",
    "\n",
    "このトレードオフのバランスを最適化する手法の一つが、Epsilon-Greedy法です。Epsilonの値を確率として与え、例えばepsilon=0.2であれば、探索が20%で活用は80%になります。\n",
    "\n",
    "ここでは、単純なコイントスゲームを多腕バンディット問題（Multi-armed bandit problem）として実装していきます。\n",
    "\n",
    "### 実装\n",
    "\n",
    "下記を確認してください。\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/EL/notebooks/Epsilon%26Greedy.ipynb\n",
    "\n",
    "クラスCoinToss はこのゲームの環境を定義しています。\n",
    "CoinToss.head_probs = [0.2, 0.5, 0.1] と入力すると、３つのそれぞれ表がでる確率がことなあるコインを選んでトスするゲーム環境になります。\n",
    "コインが表になったら、reward=1.0を獲得し、裏なら0.0になります。\n",
    "CoinToss.max_episode_steps=30なら30回コインを選んでトスします。つまりエピソードを設定するということです。\n",
    "CoinToss.step(action)で、一回のトスでactionを実行します。actionはコインの選択なので、CoinToss.head_probs[action]で選択したコインの真の確率を参照できます。\n",
    "\n",
    "この環境のCoinToss.head_probsの情報を、エージェントは探索しながら推定していくことになります。\n",
    "\n",
    "そのエージェントは、クラスEpsilonGreedyAgentとして定義されています。\n",
    "引数のepsilonが探索と活用の割合を決めます。\n",
    "エージェントが実行するCoinToss.stepの結果から、EpsilonGreedyAgent.Vを更新していきます。\n",
    "\n",
    "EpsilonGreedyAgent.policy が　Epsilon-Greedy法の根幹をなす処理です。\n",
    "その処理は簡単で、確率epsironでランダムでアクションを選択し、Vを更新します。それ以外は、Vが最大値をとるactionを選択します。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 計画の修正を実績から行うか、予測で行うか：Monte Carlo vs Temporal Difference\n",
    "\n",
    "\n",
    "計画の修正を実績で行う手法をモンテカルロ法（Monte Carlo Methods)と呼びますが、予測で行う手法もあります。\n",
    "その一つとしてTD法（Temporal Difference Learning）を紹介します。\n",
    "さらに、そのハイブリッドな手法としてMulti-step LearningとTD($\\lambda$)法を紹介します。\n",
    "\n",
    "その大きな違いは、計画の修正のタイミングです。\n",
    "\n",
    "１エピソードが終了してから、その実績に基づいて計画を修正するのをモンテカルロ法と言います。\n",
    "１回ごとの行動の直後に随時修正を行うのをTD法（厳密に言えばTD(0)法）と言います。\n",
    "\n",
    "計画の修正（つまり、価値の見積もり修正）について、厳密に確認してみましょう。\n",
    "\n",
    "時点tで状況sであった時、何らかのポリシーに基づいて行動選択をおこなった場合に報酬rが得られて、状態が$s'$に変遷しました。すなわち、下図のような状態になったとします。\n",
    "\n",
    "![img/fig3.4.png](img/fig3.4.png)\n",
    "\n",
    "この$r+\\gamma V(s')-V(s)$が見積もりから行動した後の誤差であり、TD誤差（Temporal Difference Error)と呼ばれます。\n",
    "これが「経験」の正体です。\n",
    "\n",
    "この経験による修正は、下記の処理となります。\n",
    "\n",
    "$$\n",
    "    V(s) ← V(s) + \\alpha (r + \\gamma V(s') - V(s))　\\;\\; ...(TD(0)法)\n",
    "$$\n",
    "\n",
    "学習率$\\alpha$は、誤差を学習結果に含める度合いを調整します。\n",
    "\n",
    "上記のTD(0)法では、1stepごとにVを更新をしましたが、これを2stepごと、3stepごとへと変更できます。複数stepごとに更新する手法を Multi-step Learning と呼びます。\n",
    "\n",
    "また、各stepでの実際の価値を合成して誤差を計算する、という手法もあります。これがTD($\\lambda$)法です。（下図参照）\n",
    "\n",
    "![img/fig3.5.png](img/fig3.5.png)\n",
    "\n",
    "![img/fig3.5.2.png](img/fig3.5.2.png)\n",
    "\n",
    "TD($\\lambda$)法では、$\\lambda = 0$では単なるTD法となりの直前の結果しか考慮しません。\n",
    "また$\\lambda = 1$では最終結果のみを考慮するのでモンテカルロ法と同じになります。\n",
    "$\\lambda$を増やして、１に近づくほど、長いステップでの経験を重視するようになります。\n",
    "\n",
    "### 実装\n",
    "\n",
    "本節では、モンテカルロ法とTD法をしようしたQ-learningを実装します。\n",
    "\n",
    "その前に、実装のベースになるクラス（エージェントや環境など）を紹介します。\n",
    "\n",
    "以下のコードは、エージェントを定義しています。\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/EL/el_agent.py\n",
    "\n",
    "クラスELAgentがエージェントを定義しており、そのポリシーELAgent.policyはEpsilon-Greedy法が実装されています。\n",
    "状態評価はELAgent.Q（Vではありません）で管理しています。\n",
    "エージェントが獲得した報酬はELAgent.reward_logで保存されます。\n",
    "ELAgent.show_reward_logでは報酬のログをグラフなどで表示します。\n",
    "\n",
    "続いて、以下のコードで環境を定義しています。\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/EL/frozen_lake_util.py\n",
    "\n",
    "![img/fig3.6.png](img/fig3.6.png)\n",
    "\n",
    "\n",
    "以上でベースとなるクラスは定義できました。\n",
    "\n",
    "各手法ごとの実装は以下の通りです。\n",
    "\n",
    "#### モンテカルロ法\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/EL/monte_carlo.py\n",
    "\n",
    "価値は変数Gとして計算され、エピソードが終了してから更新するモンテカルロ法では、以下の数式となります。\n",
    "\n",
    "![img/monte_carlo_G.png](img/monte_carlo_G.png)\n",
    "\n",
    "\n",
    "#### Q-learning(Q学習)法　（代表的なTD法）\n",
    "\n",
    "状態における行動の評価値（$Q(s, a)$）を慣例的にQ値といい、その関数をQ-functionと言います。また、プログラミングの実装上で、Q[s][a]のようにテーブルで管理する場合はQ-tableと言います。\n",
    "\n",
    "以下が、その具体的な実装になります。\n",
    "\n",
    "https://github.com/icoxfog417/baby-steps-of-rl-ja/blob/master/EL/q_learning.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3.3 経験を状態評価、戦略どちらの更新に利用するか\n",
    "\n",
    "得られた経験を「状態評価」の更新に使う（Valueベース）か、「戦略」の更新に使う（Policyベース）かという選択があります。\n",
    "いずれも、経験（＝TD誤差）を学習する点は同じですが、適応先が異なります。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 仮に、広告最適化の例で具体的に考えてみましょう。\n",
    "    \n",
    "状況sはユーザーIDに相当します。ここでは5個のユーザーIDがあるとします。\n",
    "（もちろん、現実的には「どのブラウザがどのWebページを何時にみているか、を扱うので、状況sはブラウザクッキーID(=ユーザーID)、URL、タイムスタンプとなります。この解説では単純化するために、ユーザーIDのみを状況sとして扱うことにします。）\n",
    "\n",
    "アクションaは表示する広告IDの選択です。映画、家電製品、旅行、化粧品、、、多くの広告IDから表示すべき物を選択します。ここでは10個の広告IDがあるとします。\n",
    "\n",
    "報酬はクリックするかしないかです。議論をシンプルにするために、広告IDごとのクリック単価は同じとします。\n",
    "\n",
    "よって、Q(s,a) をユーザーID(s)に広告ID(a)を表示した時のクリック率です。これを行列として管理するとします。\n",
    "\n",
    "今回は、エージェントが採用するポリシーは、Q(s,a)が最大になるようなaを選択する、とします。\n",
    "\n",
    "##### t=0の時\n",
    "\n",
    "過去の広告の平均クリック率を期待値にして、十分に分散が小さな乱数で、Q(s,a)にクリック率を割り当てます。\n",
    "\n",
    "#### t=1の時: $s_1=3$ (user_id=3) が発生\n",
    "\n",
    "\n",
    "Q(3, a)は次元数が5のベクトルです。ユーザーID=3に5つの広告IDを表示した時の推定クリック率を表しています。Q値が最大になるようにaを選択しますが、この時点では乱数選択になります。\n",
    "\n",
    "ここでは、$a_1=４$が選択されたとしましょう。\n",
    "\n",
    "すると、ユーザーがクリックしなかった。なので、$r_1=0$が観測された事になります。\n",
    "\n",
    "よって、以下の観測が得られました。\n",
    "\n",
    "$$\n",
    "t=1, s_1=3, a_1=4, r_1=0\n",
    "$$\n",
    "\n",
    "この観測と式(2)に基づいて、Q(s,a)行列を更新します。式(2)にこの観測結果を当てはめると以下のようになります。\n",
    "\n",
    "$$\n",
    "Q(s_t, a_t) ←  (1-\\alpha)Q(s_t, a_t) + \\alpha (r_{t+1} + \\gamma \\max_{a_{t+1}}(Q(s_{t+1},a_{t+1}))) \\:\\: ...(2)\n",
    "$$\n",
    "\n",
    "$$↓\\:t=1\\:↓$$\n",
    "\n",
    "$$\n",
    "Q(s_0, a_0) ←  (1-\\alpha)Q(s_0, a_0) + \\alpha (r_1 + \\gamma \\max_{a_1}(Q(s_1,a_1)))\n",
    "$$\n",
    "\n",
    "$$↓\\:s_1=s_0=3; \\: a_1=a_0=4; \\: r_1=0\\:↓$$\n",
    "\n",
    "$$\n",
    "Q(3, 4) ←  (1-\\alpha)Q(3, 4) + \\alpha (0 + \\gamma \\max_{a_{t+1}}(Q(3,a_{t+1}))) \n",
    "$$\n",
    "\n",
    "$$↓\\:仮定から\\max_{a_{t+1}}(Q(3,a_{t+1}))) = Q(3,4)\\:↓$$\n",
    "\n",
    "$$\n",
    "Q(3, 4) ←  (1-\\alpha)Q(3, 4) + \\alpha (0 + \\gamma Q(3,4))\n",
    "$$\n",
    "\n",
    "#### t=2の時: $s_2=1$ (user_id=1) が発生\n",
    "\n",
    "t=1の時と同じことを繰り返して、a_tを選択し、観測された結果から、Q(s,a)を更新していく。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
